{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters-21578 News classification\n",
    "\n",
    "#### Author: Qihao LIU\n",
    "Date: 02/01/2021\n",
    "\n",
    "Reuters-21578 is arguably the most commonly used collection for text classification. It contains structured information about newswire articles that can be assigned to several classes, making it a multi-label problem. \n",
    "It has a highly skewed distribution of documents over categories, where a large proportion of documents belong to few topics. The collection originally consisted of 21,578 documents but a subset and split is traditionally used. \n",
    "The most common split is Mod-Apte which only considers categories that have at least one document in the training set and the test set. The Mod-Apte split has 90 categories with a training set of 7769 documents and a test set of 3019 documents.This method of splitting can directly been used by library nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning -  Data Preparation\n",
    "\n",
    "### 1.1 Introduction\n",
    "\n",
    "This part goes through a necessary step of any data science project - data cleaning. Data cleaning is a time consuming and unenjoyable task. Feeding dirty data into a model will give us results that are meaningless.\n",
    "\n",
    "Processing:\n",
    "\n",
    "* Getting the data - in this case, we'll be scraping data from the 22 sgm files containing the 21 578 Reuters articles\n",
    "* Cleaning the data - we will walk through popular text pre-processing techniques\n",
    "* Organizing the data - we will organize the cleaned data into a way that is easy to input into other algorithms\n",
    "\n",
    "The output of this part - organized data in two standard text formats:\n",
    "\n",
    "* Corpus - a collection of text;\n",
    "* Document-Term Matrix - Word2Vec in matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Problem Statement\n",
    "\n",
    "The Reuters-21578 dataset contains 21 578 financial articles tagged with topics.\n",
    "There are 135 different topics, but this exercise will focus on only 5 of them:\n",
    "-\tMoney/Foreign Exchange (MONEY-FX)\n",
    "-\tShipping (SHIP)\n",
    "-\tInterest Rates (INTEREST)\n",
    "-\tMergers/Acquisitions (ACQ)\n",
    "-\tEarnings and Earnings Forecasts (EARN)\n",
    "\n",
    "So this is a supervised classification, we do not need to use topic modelling techniques for modelling topics form data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Getting The Data\n",
    "\n",
    "There are two ways to get the reuter21578 data:\n",
    "\n",
    "* Download the collection and parse the multiple SGML files in order to recreate the original dataset;\n",
    "* Or, much easier way with the NLTK library which has the reuters corpus already available. \n",
    "\n",
    "Libraies:\n",
    "Using BeautifulSoup libray to help us pick out certain sections from sgml files in order to parse all SGML files, removing all unwanted tags and a simple regex to strip the ending signature.\n",
    "\n",
    "The following code shows how to deal with the original dataset sgml but actually using NLTK library with Mode-Apte will be efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected categories\n",
    "selected_categories = [\"to_money-fx\",\"to_ship\",\"to_interest\",\"to_acq\",\"to_earn\"] \n",
    "# Category files\n",
    "category_files = { 'to_': ('Topics', 'all-topics-strings.lc.txt')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name    Type  Newslines\n",
      "0      to_acq  Topics          0\n",
      "1     to_alum  Topics          0\n",
      "2  to_austdlr  Topics          0\n",
      "3  to_austral  Topics          0\n",
      "4   to_barley  Topics          0\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "# Create category dataframe\n",
    "\n",
    "# Read all categories\n",
    "category_data = []\n",
    "\n",
    "# Newsline folder and format\n",
    "data_folder = 'C:/Users/LQH/Desktop/CA CIB/reuters21578/'\n",
    "\n",
    "# Building dataframe for visualising topics details like numbers of documents to this topic\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0], 0])\n",
    "\n",
    "# Create category dataframe\n",
    "news_categories = DataFrame(data=category_data, columns=['Name', 'Type', 'Newslines'])\n",
    "print(news_categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#update the numbres of documents with same topic (count nbs)\n",
    "def update_frequencies(categories):\n",
    "    for category in categories:\n",
    "        idx = news_categories[news_categories.Name == category].index[0]\n",
    "        f = news_categories.get_value(idx, 'Newslines')\n",
    "        news_categories.set_value(idx, 'Newslines', f+1)\n",
    "\n",
    "#building vector Y(labels) for each document to represent whether the documents relative to the topic in \n",
    "#list [\"to_money-fx\",\"to_ship\",\"to_interest\",\"to_acq\",\"to_earn\"]\n",
    "def to_category_vector(categories, target_categories):\n",
    "    vector = np.zeros(len(target_categories)).astype(np.float32)\n",
    "    for i in range(len(target_categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-000.sgm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\app\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  import sys\n",
      "C:\\app\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-001.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-002.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-003.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-004.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-005.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-006.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-007.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-008.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-009.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-010.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-011.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-012.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-013.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-014.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-015.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-016.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-017.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-018.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-019.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-020.sgm...\n",
      "Start parsing C:/Users/LQH/Desktop/CA CIB/reuters21578\\reut2-021.sgm...\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "from glob import glob\n",
    "\n",
    "# Parse SGML files\n",
    "document_X = {}\n",
    "document_Y = {}\n",
    "\n",
    "# removing Special symbol like < for reading the specific part in the files\n",
    "#'&amp;','&lt;','&gt;'\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "f_list = glob('C:/Users/LQH/Desktop/CA CIB/reuters21578/*.sgm')\n",
    "\n",
    "# Iterate all files\n",
    "for filename in f_list:\n",
    "    print('Start parsing {0}...'.format(filename))\n",
    "    file = open(filename, 'rb')\n",
    "    content = BeautifulSoup(file.read().lower())\n",
    "    file.close()\n",
    "    for newsline in content('reuters'):\n",
    "        document_categories = []           \n",
    "        # News-line Id\n",
    "        document_id = newsline['newid']\n",
    "        # Extracting document body\n",
    "        document_body = strip_tags(str(newsline('text')[0])).replace('reuter\\n&#3;', '')\n",
    "        document_body = unescape(document_body)\n",
    "        # News-line categories\n",
    "        topics = newsline.topics.contents\n",
    "        for topic in topics:\n",
    "            document_categories.append('to_' + strip_tags(str(topic)))                \n",
    "        # Create new document    \n",
    "        update_frequencies(document_categories)\n",
    "        #Filter the documents in list of [\"to_money-fx\",\"to_ship\",\"to_interest\",\"to_acq\",\"to_earn\"] \n",
    "        if sum(to_category_vector(document_categories, selected_categories))>=1.0:\n",
    "            document_Y[document_id] = to_category_vector(document_categories, selected_categories)\n",
    "            document_X[document_id] = document_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "american exchange introduces institutional index\n",
      "    new york, oct 19 - the american stock exchange said it has\n",
      "introduced options with expirations of up to three years on the\n",
      "institutional index.\n",
      "    with the ticker symbol <xii>, the index is a guage of the\n",
      "core equity holdings of the nation's largest institutions, the\n",
      "exchange explained.\n",
      "    the new listings represent the first long-term options to\n",
      "be traded by the amex, it added.\n",
      "    it said the long-term institutional index options began\n",
      "trading monday with expirations of december 1988 <xiv> and\n",
      "december 1989 <xix>.\n",
      "   \n",
      "    the amex said a third long-term option with an expiration\n",
      "of december 1990 will begin trading following the december 1987\n",
      "expiration.\n",
      "    it said strike prices on the long-term options have been\n",
      "set at 50 point intervals with initial strikes of 250, 300 and\n",
      "350. to avoid conflicting strike price codes, the 350 stike\n",
      "prices will carry the ticker symbols <xvv> for the option\n",
      "expiring in december 1988 and <xvx> for the option expiring in\n",
      "december 1989.\n",
      " reuter\n",
      "<d>gold</d>\n"
     ]
    }
   ],
   "source": [
    "print(document_body)\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7824\n",
      "7824\n"
     ]
    }
   ],
   "source": [
    "print(len(document_Y))\n",
    "#print(document_Y)\n",
    "print(len(document_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Newslines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>to_earn</td>\n",
       "      <td>Topics</td>\n",
       "      <td>3987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>to_acq</td>\n",
       "      <td>Topics</td>\n",
       "      <td>2448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>to_money-fx</td>\n",
       "      <td>Topics</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>to_crude</td>\n",
       "      <td>Topics</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>to_grain</td>\n",
       "      <td>Topics</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>to_trade</td>\n",
       "      <td>Topics</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>to_interest</td>\n",
       "      <td>Topics</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>to_wheat</td>\n",
       "      <td>Topics</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>to_ship</td>\n",
       "      <td>Topics</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>to_corn</td>\n",
       "      <td>Topics</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name    Type  Newslines\n",
       "35       to_earn  Topics       3987\n",
       "0         to_acq  Topics       2448\n",
       "73   to_money-fx  Topics        801\n",
       "28      to_crude  Topics        634\n",
       "45      to_grain  Topics        628\n",
       "126     to_trade  Topics        552\n",
       "55   to_interest  Topics        513\n",
       "130     to_wheat  Topics        306\n",
       "108      to_ship  Topics        305\n",
       "19       to_corn  Topics        254"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(document_X[\"5\"])\n",
    "#topic_list = [\"to_money-fx\",\"to_ship\",\"to_interest\",\"to_acq\",\"to_earn\"] \n",
    "news_categories.sort_values(by='Newslines', ascending=False, inplace=True)\n",
    "news_categories.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the numbers of documents relative to the topic that we consider in this exercise, where the sample is not evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Newslines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>to_earn</td>\n",
       "      <td>Topics</td>\n",
       "      <td>3987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>to_acq</td>\n",
       "      <td>Topics</td>\n",
       "      <td>2448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>to_money-fx</td>\n",
       "      <td>Topics</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>to_interest</td>\n",
       "      <td>Topics</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>to_ship</td>\n",
       "      <td>Topics</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name    Type  Newslines\n",
       "35       to_earn  Topics       3987\n",
       "0         to_acq  Topics       2448\n",
       "73   to_money-fx  Topics        801\n",
       "55   to_interest  Topics        513\n",
       "108      to_ship  Topics        305"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_categories.loc[[35,0,73,55,108]]\n",
    "#print(document_X.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Cleaning The Data\n",
    "\n",
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "We're going to execute just the common cleaning steps here:\n",
    "- Make text all lower case\n",
    "- Remove punctuation\n",
    "- Remove numerical values\n",
    "- Remove common non-sensical text (/n)\n",
    "- Tokenize text\n",
    "- Remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer('english')\n",
    "#Make text lowercase, remove text in square brackets,remove punctuation, remove \\n and remove words containing numbers.\n",
    "def tokenize(text):\n",
    "    min_length = 3\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "   # words = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    tokens =(list(map(lambda token: PorterStemmer().stem(token),words)))\n",
    "   # tokens = ' '.join(stemmer.stem(i) for i in words)\n",
    "    p = re.compile('[\\'a-zA-Z]+') #Matches one or more alphabetical characters.\n",
    "   # filtered_tokens = ' '.join(token for token in tokens if (p.match(token) and len(token)>=min_length))\n",
    "    filtered_tokens = list(filter(lambda token: p.match(token) and len(token)>=min_length,tokens))\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7824\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Tokenized document collection\n",
    "newsline_documents = []\n",
    "word_nb = 0\n",
    "# Tokenize\n",
    "for key in document_X.keys():\n",
    "    newsline_documents.append(tokenize(document_X[key]))\n",
    "    word_nb += len(tokenize(document_X[key]))\n",
    "number_of_documents = len(document_X)\n",
    "print(number_of_documents)\n",
    "print(word_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "def tokenize1(text):\n",
    "    \"\"\"function to clean text by removing punctuations, and numbers\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    :param text: a string\n",
    "    \n",
    "    :returns: string with punctuations and numbers removed\n",
    "    \"\"\"\n",
    "    min_length = 2\n",
    "    stopwords_set= set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text = text.replace('\\n',' ').lower().strip()\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \", text).split()\n",
    "    text = ' '.join(stemmer.stem(i) for i in text)\n",
    "    stemmed = ' '.join([word for word in text.split() if word not in stopwords_set and len(word)>=min_length])\n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7824\n",
      "3401172\n"
     ]
    }
   ],
   "source": [
    "# Tokenized document collection\n",
    "newsline_documents = []\n",
    "word_nb = 0\n",
    "# Tokenize\n",
    "for key in document_X.keys():\n",
    "    newsline_documents.append(tokenize1(document_X[key]))\n",
    "    word_nb += len(tokenize1(document_X[key]))\n",
    "number_of_documents = len(document_X)\n",
    "print(number_of_documents)\n",
    "print(word_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owen minor inc obod rais qtli dividend richmond va feb qtli div eight cts vs cts prior pay march record march reuter\n"
     ]
    }
   ],
   "source": [
    "print(newsline_documents[10])\n",
    "#print(document_X[\"10\"])\n",
    "# 7824\n",
    "# 3415142\n",
    "# 7824\n",
    "# 489300\n",
    "#document_X[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF transformation in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\app\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn TFIDF processing time: 1.52021 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\app\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "body_list = newsline_documents\n",
    "start = time.clock()\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(newsline_documents)\n",
    "print (\"sklearn TFIDF processing time: {0:.5f} s\".format(time.clock() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7824, 18249)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_tfidf = vectorizer.transform(newsline_documents)\n",
    "matrix_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_categories = len(selected_categories)\n",
    "topic_class = np.zeros(shape=(number_of_documents, num_categories))\n",
    "for idx, key in enumerate(document_Y.keys()):\n",
    "    topic_class[idx, :] = document_Y[key]\n",
    "topic_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5476, 18249)\n",
      "(5476, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#split train and test dataset with ratio of 0.3\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(matrix_tfidf, topic_class, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16502)\t0.0630335651497171\n",
      "  (0, 16120)\t0.14835008665637345\n",
      "  (0, 15592)\t0.07553828422534323\n",
      "  (0, 15462)\t0.10061046790307208\n",
      "  (0, 14594)\t0.06026518968723119\n",
      "  (0, 14591)\t0.08036762016998687\n",
      "  (0, 14524)\t0.061006377090775644\n",
      "  (0, 14494)\t0.2043151895341332\n",
      "  (0, 14070)\t0.08589557766381234\n",
      "  (0, 13743)\t0.32396102634753565\n",
      "  (0, 13637)\t0.019531329156803508\n",
      "  (0, 13610)\t0.06048435177120554\n",
      "  (0, 13157)\t0.09917668555384727\n",
      "  (0, 13141)\t0.08122198744509536\n",
      "  (0, 12930)\t0.12852100619352408\n",
      "  (0, 12752)\t0.06136301929722827\n",
      "  (0, 12667)\t0.15946153342884392\n",
      "  (0, 12179)\t0.10455567067064062\n",
      "  (0, 12053)\t0.040828144390355106\n",
      "  (0, 11803)\t0.09756131485583543\n",
      "  (0, 11503)\t0.08726465611651313\n",
      "  (0, 11482)\t0.17567377673879003\n",
      "  (0, 9808)\t0.055534656072440285\n",
      "  (0, 9673)\t0.06654663157461375\n",
      "  (0, 9201)\t0.09756131485583543\n",
      "  :\t:\n",
      "  (5475, 14591)\t0.13925626617681716\n",
      "  (5475, 13639)\t0.16596120473504186\n",
      "  (5475, 13637)\t0.03384273375389291\n",
      "  (5475, 13610)\t0.10480371288795148\n",
      "  (5475, 13595)\t0.16111672039172123\n",
      "  (5475, 13201)\t0.22874071386728534\n",
      "  (5475, 12997)\t0.1451770453631435\n",
      "  (5475, 12441)\t0.28852497703406443\n",
      "  (5475, 12388)\t0.21535462762298596\n",
      "  (5475, 12225)\t0.15101901737096157\n",
      "  (5475, 12138)\t0.10082408213898586\n",
      "  (5475, 11452)\t0.11006393123533419\n",
      "  (5475, 11256)\t0.07695122181286806\n",
      "  (5475, 11116)\t0.19980781090363042\n",
      "  (5475, 11015)\t0.1889289359264432\n",
      "  (5475, 10732)\t0.20952257747894534\n",
      "  (5475, 10467)\t0.053011332916196106\n",
      "  (5475, 9058)\t0.2600117429983184\n",
      "  (5475, 5716)\t0.2669555481475594\n",
      "  (5475, 3893)\t0.36670681163345725\n",
      "  (5475, 3265)\t0.19453506895120729\n",
      "  (5475, 3194)\t0.08862043577885702\n",
      "  (5475, 2808)\t0.12372925720673648\n",
      "  (5475, 226)\t0.18268614927822624\n",
      "  (5475, 104)\t0.1319872660593192\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def linearsvc_classifier(train_docs, train_labels):\n",
    "    classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "    classifier.fit(train_docs, train_labels)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def naive_classifier(train_docs, train_labels):\n",
    "    classifier = OneVsRestClassifier(MultinomialNB(alpha=0.01))\n",
    "    classifier.fit(train_docs, train_labels)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# estimator:估计方法对象(分类器)\n",
    "# X：数据特征(Features)\n",
    "# y：数据标签(Labels)\n",
    "# soring：调用方法(包括accuracy和mean_squared_error等等)\n",
    "# cv：几折交叉验证\n",
    "# n_jobs：同时工作的cpu个数（-1代表全部）\n",
    "\n",
    "def cross_validation(clf):\n",
    "    num_folds = 10\n",
    "    # logger.info(\"Cross validation\")\n",
    "    print(\"Cross validation\")\n",
    "    scores = cross_val_score(clf,\n",
    "                             X_train, Y_train,\n",
    "                             cv=num_folds,\n",
    "                             n_jobs=-1,\n",
    "                             verbose=0)\n",
    "    print(f\"Real risk by {num_folds}-fold CV : {scores.mean():.2} (+/- {scores.std():.2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='micro')\n",
    "    recall = recall_score(test_labels, predictions, average='micro')\n",
    "    f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    precision = precision_score(test_labels, predictions,average='macro',zero_division=1)\n",
    "    recall = recall_score(test_labels, predictions, average='macro',zero_division=1)\n",
    "    f1 = f1_score(test_labels, predictions, average='macro',zero_division=1)\n",
    "\n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation\n",
      "Real risk by 10-fold CV : 0.94 (+/- 0.0067)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(OneVsRestClassifier(LinearSVC(random_state=42)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.9753, Recall: 0.9680, F1-measure: 0.9716\n",
      "Macro-average quality numbers\n",
      "Precision: 0.9510, Recall: 0.9273, F1-measure: 0.9386\n"
     ]
    }
   ],
   "source": [
    "#documents = reuters.fileids()\n",
    "model_svm = linearsvc_classifier(X_train, Y_train)\n",
    "predictions_svm = model_svm.predict(X_test)\n",
    "evaluate(Y_test, predictions_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.9935, Recall: 0.9954, F1-measure: 0.9944\n",
      "Macro-average quality numbers\n",
      "Precision: 0.9863, Recall: 0.9887, F1-measure: 0.9875\n"
     ]
    }
   ],
   "source": [
    "model_svm = linearsvc_classifier(X_train, Y_train)\n",
    "predictions_svm = model_svm.predict(X_train)\n",
    "evaluate(Y_train, predictions_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navie bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation\n",
      "Real risk by 10-fold CV : 0.84 (+/- 0.013)\n"
     ]
    }
   ],
   "source": [
    "cross_validation(OneVsRestClassifier(MultinomialNB(alpha=0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.8955, Recall: 0.9298, F1-measure: 0.9123\n",
      "Macro-average quality numbers\n",
      "Precision: 0.8391, Recall: 0.9420, F1-measure: 0.8802\n"
     ]
    }
   ],
   "source": [
    "model_naive = naive_classifier(X_train, Y_train)\n",
    "predictions_navie = model_naive.predict(X_test)\n",
    "evaluate(Y_test, predictions_navie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.9399, Recall: 0.9834, F1-measure: 0.9611\n",
      "Macro-average quality numbers\n",
      "Precision: 0.8913, Recall: 0.9905, F1-measure: 0.9317\n"
     ]
    }
   ],
   "source": [
    "model_naive = naive_classifier(X_train, Y_train)\n",
    "predictions_navie = model_naive.predict(X_train)\n",
    "evaluate(Y_train, predictions_navie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Organizing The Data\n",
    "Organized data in two standard text formats:\n",
    "\n",
    "* Corpus - a collection of text\n",
    "* Document-Term Matrix - using Word2Vec\n",
    "\n",
    "Using One-Vs-Rest categorization method, using Word2Vec  (implemented by Gensim), which is much more effective than a standard bag-of-words or Tf-Idf approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features = 100\n",
    "\n",
    "# Create new Gensim Word2Vec model\n",
    "w2v_model = Word2Vec(newsline_documents, size=num_features, min_count=1, window=10, workers=cpu_count())\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.save(data_folder + 'reuters.word2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\app\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\app\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Limit each newsline to a fixed number of words\n",
    "document_max_num_words = 100\n",
    "\n",
    "num_categories = len(selected_categories)\n",
    "X = np.zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(np.float32)\n",
    "Y = np.zeros(shape=(number_of_documents, num_categories)).astype(np.float32)\n",
    "\n",
    "empty_word = np.zeros(num_features).astype(np.float32)\n",
    "\n",
    "for idx, document in enumerate(newsline_documents):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break            \n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                X[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                X[idx, jdx, :] = empty_word\n",
    "\n",
    "for idx, key in enumerate(document_Y.keys()):\n",
    "    Y[idx, :] = document_Y[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classifying Reuters \n",
    "\n",
    "\n",
    "In order to classify the collection, we have to apply a number of steps which are standard for the majority of classification problems:\n",
    "\n",
    "* Define our training and testing subsets to make sure that we do not evaluate with documents that the system has learnt from. In our case, split train and test dataset with ratio of 0.3.\n",
    "* Represent all the documents in each subset.\n",
    "* Train a classifier on the represented training data.\n",
    "* Predict the labels for each one of the represented testing documents.\n",
    "* Compare the real and predicted document labels to evaluate our solution.\n",
    "\n",
    "Model:\n",
    "* Using model linear SVM (LinearSVC), this model has traditionally produced good quality with text classification problems;\n",
    "* and Gaussian Naive Bayes (GaussianNB).\n",
    "The problem we are solving has a multi-label nature, we have to train our model (which is binary by nature) N times, once per category, where the negative cases will be the documents in all the other categories. This allows our model to make a binary decision per category and produce multi-label results. This can be done with the OneVsRestClassifier object in Scikit-learn. This step might change depending on the estimator like kNN which is multi-label by nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5476, 10000)\n",
      "(5476, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "nsamples, nx, ny = X.shape\n",
    "X_2dim = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "#split train and test dataset with ratio of 0.3\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_2dim, Y, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def linearsvc_classifier(train_docs, train_labels):\n",
    "    classifier = OneVsRestClassifier(LinearSVC(penalty='l2', loss='squared_hinge',random_state=42))\n",
    "    classifier.fit(train_docs, train_labels)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='micro')\n",
    "    recall = recall_score(test_labels, predictions, average='micro')\n",
    "    f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    precision = precision_score(test_labels, predictions,average='macro',zero_division=1)\n",
    "    recall = recall_score(test_labels, predictions, average='macro',zero_division=1)\n",
    "    f1 = f1_score(test_labels, predictions, average='macro',zero_division=1)\n",
    "\n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\app\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\app\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\app\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.9270, Recall: 0.9144, F1-measure: 0.9207\n",
      "Macro-average quality numbers\n",
      "Precision: 0.8622, Recall: 0.8362, F1-measure: 0.8487\n"
     ]
    }
   ],
   "source": [
    "#documents = reuters.fileids()\n",
    "model = linearsvc_classifier(X_train, Y_train)\n",
    "predictions = model.predict(X_test)\n",
    "evaluate(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def naive_classifier(train_docs, train_labels):\n",
    "    classifier = OneVsRestClassifier(GaussianNB())\n",
    "    classifier.fit(train_docs, train_labels)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5476, 5)\n",
      "(5476, 10000)\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4318, Recall: 0.7328, F1-measure: 0.5434\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3475, Recall: 0.6624, F1-measure: 0.4259\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "model = naive_classifier(X_train, Y_train)\n",
    "predictions = model.predict(X_test)\n",
    "evaluate(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RandomForestRegressor model\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# RF = RandomForestRegressor(n_estimators=10, criterion=\"mae\", max_depth=3)\n",
    "# RF.fit(X_train, Y_train)\n",
    "# predictions = model.predict(X_test)\n",
    "# evaluate(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Predictive analysis\n",
    "\n",
    "Our samples are not evenly distributed:\n",
    "\tName\t     Type\tNewslines\n",
    "35\tto_earn\t     Topics\t3987\n",
    "0\tto_acq\t     Topics\t2448\n",
    "73\tto_money-fx\t  Topics\t801\n",
    "55\tto_interest \tTopics\t513\n",
    "108\tto_ship\t     Topics\t305\n",
    "\n",
    "So Micro-average is better performed because it chonsider this problem in the formula:\n",
    "* Linear SVM (LinearSVC):\n",
    "Micro-average quality numbers\n",
    "Precision: 0.9270, Recall: 0.9144, F1-measure: 0.9207\n",
    "Macro-average quality numbers\n",
    "Precision: 0.8622, Recall: 0.8362, F1-measure: 0.8487\n",
    "\n",
    "* Gaussian Naive Bayes (GaussianNB):\n",
    "Micro-average quality numbers\n",
    "Precision: 0.4318, Recall: 0.7328, F1-measure: 0.5434\n",
    "Macro-average quality numbers\n",
    "Precision: 0.3475, Recall: 0.6624, F1-measure: 0.4259\n",
    "\n",
    "We could see Linear SVM preform much better than Naive Bayes in this case, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Topic Modeling\n",
    "\n",
    "In this case, our problem is supervised. Generally, NLP problems are unsupervised which need to do the topic modeling. The ultimate goal of topic modeling is to find various topics that are present in the corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "Method: Latent Dirichlet Allocation (LDA), which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, we need to provide \n",
    "* a document-term matrix;\n",
    "* and the number of topics we would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, our job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, we can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Getting reuters dataset from NLTK library\n",
    "\n",
    "The most common split is Mod-Apte which only considers categories that have at least one document in the training set and the test set. The Mod-Apte split has 90 categories with a training set of 7769 documents and a test set of 3019 documents.This method of splitting can directly been used by library nltk.\n",
    "\n",
    "Useful blog:\n",
    "https://martin-thoma.com/nlp-reuters/\n",
    "https://ana.cachopo.org/datasets-for-single-label-text-categorization\n",
    "https://towardsdatascience.com/analysis-and-visualization-of-unstructured-text-data-2de07d9adc84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
